# -*- coding: utf-8 -*-
"""random forest specific file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nJp_PrBueXRkQ4Yv4ux2GI5Pz9vy73Ef

# 4.F1 Prediction Project -Model Notebook V1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from tempfile import mkdtemp
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import joblib

# Load DataFrame
file = 'model_data.csv'
df = pd.read_csv(file)

df.info(verbose=True, max_cols=None)

pd.set_option('display.max_columns', None)

df.head()

df = df.drop('race_index', axis=1)

df.head()

# Perform train-test split
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)  # You can adjust the test_size as needed

# Save training and testing data to separate CSV files
train_data.to_csv('train_data.csv', index=False)
test_data.to_csv('test_data.csv', index=False)

# Get the normalized value counts for the 'race_win' column and multiply by 100 to get percentages
value_counts = train_data['race_win'].value_counts(normalize=True) * 100

# Create the bar chart
plt.figure()
plt.bar(value_counts.index.astype(str), value_counts.values)
plt.xlabel('Race Win or Not')
plt.ylabel('Percentage of Total Training Set (%)')
plt.xticks([0, 1], ['Did not Win', 'Did Win'])
plt.title('Distribution of "Win" rows in dataset')
plt.show()

# Define X and Y for the training model
X_train = train_data.drop('race_win', axis=1)
y_train = train_data['race_win']

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Set up a directory to cache the pipeline results
cachedir = mkdtemp()

# Set up a pipeline
# The steps here act as placeholders and will be changed when we pass the pipeline

my_pipeline = Pipeline([('scaler', StandardScaler()),
('dim_reducer', PCA()),
('model', LogisticRegression())], memory=cachedir)

# Define X and Y for the training model
X_test = test_data.drop('race_win', axis=1)
y_test = test_data['race_win']

"""# 7.  Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Initialize the classifier
classifier = RandomForestClassifier(random_state=42)

# Fit the model
classifier.fit(X_train, y_train)

# Predict the test set
y_pred_rf = classifier.predict(X_test)

# Print the classification report and confusion matrix for Non-Tuned Random Forest
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
}

# Initialize the grid search model
grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found:", grid_search.best_params_)

from sklearn.model_selection import cross_val_score

# Perform cross-validation
scores = cross_val_score(classifier, X_train, y_train, cv=5)

# Print the cross-validation scores
print("Cross-validation scores:", scores)

import joblib

# Save the model
joblib.dump(classifier, 'finalized_model.pkl')

# Load the model
loaded_model = joblib.load('finalized_model.pkl')

# Predict new data
new_data_predictions = loaded_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, new_data_predictions)
print(f"Accuracy: {accuracy}")

# Detailed classification report for Tuned Random Forest
print(confusion_matrix(y_test, new_data_predictions))
print(classification_report(y_test, new_data_predictions))

# Real scores from models
model_names = ['Logistic Regression Untuned', 'Logistic Regression Tuned', 'Decision Tree Tuned', 'Random Forest', 'Random Forest Tuned']
accuracies = [0.96, 0.95, 0.94, 0.95, 0.95]  # Replace with actual accuracy scores
macro_f1s = [0.67, 0.67, 0.67, 0.65, 0.65]  # Replace with actual macro F1 scores
precisions = [0.63, 0.46, 0.42, 0.49, 0.49]  # Replace with actual precision scores
recalls = [0.26, 0.31, 0.33, 0.24, 0.24] # Replace with actual recall scores
class_1_f1s = [0.37, 0.37, 0.37, 0.32, 0.32]  # Replace with actual '1' F1 scores

# Initialize the DataFrame with these lists
results_df = pd.DataFrame({
    'Model': model_names,
    'Model Accuracy': accuracies,
    'Macro F1': macro_f1s,
    'Class 1 Precision': precisions,
    'Class 1 Recall': recalls,
    'Class 1 F1': class_1_f1s
})

# Display the DataFrame
results_df

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
smote_df = pd.read_csv('model_data.csv')

# Separate into features and target variable
X = smote_df.drop('race_win', axis=1)
y = smote_df['race_win']

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from imblearn.over_sampling import SMOTE

# Create a SMOTE object
sm = SMOTE(random_state=42)

# Fit SMOTE to the training data
X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)

# SMOTE output is `X_train_resampled` and `y_train_resampled`
df_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)  # Use original feature names
df_resampled['target'] = y_train_resampled  # Add target variable column

# Original class distribution
original_classes, original_counts = np.unique(y_train, return_counts=True)
print("Original class distribution:", dict(zip(original_classes, original_counts)))

# Resampled class distribution
resampled_classes, resampled_counts = np.unique(y_train_resampled, return_counts=True)
print("Resampled class distribution:", dict(zip(resampled_classes, resampled_counts)))

# Get the normalized value counts for the 'race_win' column and multiply by 100 to get percentages
original_value_counts = train_data['race_win'].value_counts(normalize=True) * 100

# Get the normalized value counts for the resampled data and multiply by 100 to get percentages
resampled_value_counts = pd.Series(y_train_resampled).value_counts(normalize=True) * 100

# Create the bar chart
plt.figure(figsize=(12, 6))

# Plot for original data
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
plt.bar(original_value_counts.index.astype(str), original_value_counts.values, color='b', alpha=0.7, label='Original')
plt.xlabel('Race Win or Not')
plt.ylabel('Percentage of Total (%)')
plt.title('Original Distribution of "Win" in dataset')
plt.xticks([0, 1], ['Did not Win', 'Did Win'])
plt.legend()

# Plot for resampled data
plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
plt.bar(resampled_value_counts.index.astype(str), resampled_value_counts.values, color='r', alpha=0.7, label='Resampled')
plt.xlabel('Race Win or Not')
plt.ylabel('Percentage of Total (%)')
plt.title('Resampled Distribution of "Win" in dataset')
plt.xticks([0, 1], ['Did not Win', 'Did Win'])
plt.legend()

plt.tight_layout()
plt.show()

# Determine the indices of the new samples
new_sample_indices = [i for i in range(len(y_train_resampled)) if i >= len(y_train)]

# Display the new samples
print("Newly generated samples:")
print(X_train_resampled.iloc[new_sample_indices, :].head())

# Reduce the data to two dimensions for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_resampled)

# Plot the original samples and the synthetic samples
plt.scatter(X_train_pca[:len(y_train), 0], X_train_pca[:len(y_train), 1], label='Original')
plt.scatter(X_train_pca[len(y_train):, 0], X_train_pca[len(y_train):, 1], label='SMOTE')
plt.legend()
plt.title('Visualization of Original and SMOTE Resampled Data')
plt.xlabel('First Principal Component') # Label for the x-axis
plt.ylabel('Second Principal Component') # Label for the y-axis
plt.show()

pipe_rf = Pipeline([
    ('scl', StandardScaler()),
    ('clf', RandomForestClassifier(random_state=42))
])

# Create parameter grids for each model
# Let's try the same range of C values from earlier
c_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]

rf_param_grid = [{
    'clf__n_estimators': [100, 200, 300, 400, 500, 600, 700],  # Prefix with 'clf__'
    'clf__max_depth': [10, 20, 30, 40, 50, 60],  # Prefix with 'clf__'
}]

# Create a list to store GridSearchCV objects for each pipeline
grid_search_objects = []

# Define a list of tuples to keep model and its grid parameters together
models_grids = [(pipe_rf, rf_param_grid)]

# Perform GridSearchCV for each model
for pipe, param_grid in models_grids:
    gs = GridSearchCV(estimator=pipe,
                      param_grid=param_grid,
                      scoring='accuracy',
                      cv=5,
                      n_jobs=-1,
                      verbose=1)
    gs.fit(X_train_resampled, y_train_resampled)
    grid_search_objects.append(gs)

# Now each model has been fitted, we can inspect them
grid_dict = {0: 'Random Forest'}
for i, gs in enumerate(grid_search_objects):
    print(f"{grid_dict[i]} Test Accuracy: {gs.score(X_test, y_test)}")
    print(f"{grid_dict[i]} Best Params: {gs.best_params_}")

# grid_search_objects is a list of GridSearchCV objects
for i, gs in enumerate(grid_search_objects):
    # Predict using the best estimator found in the grid search
    y_pred = gs.best_estimator_.predict(X_test)

    # Print model name
    print(f"Model: {grid_dict[i]}")

    # Classification report
    print("Classification report:")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    print("Confusion matrix:")
    print(confusion_matrix(y_test, y_pred))
    print()  # Add an empty line for better readability

import pickle

# grid_search_objects contains GridSearchCV fitted models
for i, gs in enumerate(grid_search_objects):
    model_name = grid_dict[i].replace(" ", "_").lower()  # Creating a filename-friendly version of the model name
    filename = f"{model_name}_grid_search.pkl"

    with open(filename, 'wb') as file:
        pickle.dump(gs, file)

    print(f"Saved {grid_dict[i]} model to {filename}")





# check the csv file generated in the code below

# For each model, predict the race_win values and create a CSV
for i, gs in enumerate(grid_search_objects):
    model_name = grid_dict[i]

    # Create a DataFrame for predictions
    predictions_df = X_test.copy()
    predictions_df['Predicted_Winner'] = gs.predict(X_test)

    # Define the path to save the CSV file
    save_path = f"{model_name}_predictions.csv"

    # Export each model's prediction to a separate CSV file in the specified directory
    predictions_df.to_csv(save_path, index=False)

